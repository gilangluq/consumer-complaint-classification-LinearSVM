{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "This dataset consists of real-world consumer complaints about financial products and services. Each complaint is labeled with a specific product, framing this as a supervised text classification problem. To classify future complaints based on their content, various machine learning algorithms were employed to improve the accuracy of predictions, assigning each complaint to the appropriate product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "from io import StringIO\n",
    "import gc\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Actual models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# Visualization\n",
    "from IPython.display import display\n",
    "import seaborn as sns # used for plot interactive graph.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://storage.googleapis.com/suptech-lab-practical-data-science-public/complaints.csv.zip\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2).T\n",
    "\n",
    "# Create a new dataframe with two columns\n",
    "df1 = df[['Product', 'Consumer complaint narrative']].copy()\n",
    "\n",
    "# Remove missing values (NaN)\n",
    "df1 = df1[pd.notnull(df1['Consumer complaint narrative'])]\n",
    "\n",
    "# Renaming second column for a simpler name\n",
    "df1.columns = ['Product', 'Complaint']\n",
    "\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df1['Complaint'].notnull().sum()\n",
    "round((total/len(df)*100),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep 1.5 million (36.3%) that have an actual complaint body populated\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df1.Product.unique()).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21 distinct classes or categories (i.e., the target our model aims to predict). However, some classes overlap with others. For example, the categories Credit card and Prepaid card are also covered by the broader Credit card or prepaid card category. Now, if a new complaint is related to a credit card, the algorithm could classify it as either Credit card or Credit card or prepaid card, both of which would be correct. However, this could negatively impact the model's performance. To prevent this issue, certain category names should be revised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming categories\n",
    "df2 = df1.replace({\n",
    "    'Product':\n",
    "      {\n",
    "        'Credit reporting': 'Credit reporting, repair, or other',\n",
    "        'Credit reporting or other personal consumer reports': 'Credit reporting, repair, or other',\n",
    "        'Credit reporting, credit repair services, or other personal consumer reports': 'Credit reporting, repair, or other',\n",
    "        'Credit card': 'Credit card or prepaid card',\n",
    "        'Prepaid card': 'Credit card or prepaid card',\n",
    "        'Payday loan': 'Payday loan, title loan, personal loan, or advance loan',\n",
    "        'Payday loan, title loan, or personal loan': 'Payday loan, title loan, personal loan, or advance loan',\n",
    "        'Money transfers': 'Money transfer, virtual currency, or money service',\n",
    "        'Virtual currency': 'Money transfer, virtual currency, or money service'\n",
    "      }\n",
    "    },\n",
    "    inplace = False) # if this was a MASSIVE dataset, you may not want to make a copy but rather use df1 and inplace = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('Product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note there are only 10 complaints of 1.5 MILLION that are classified as `Debt or credit management`, so we will combine them with `Other financial service`.\n",
    "df2.replace({\n",
    "    'Product':\n",
    "      {\n",
    "        'Debt or credit management': 'Other financial service'\n",
    "      }\n",
    "    },\n",
    "    inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2.Product.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming categories\n",
    "df2 = df1.replace({\n",
    "    'Product':\n",
    "      {\n",
    "        'Credit reporting': 'Credit reporting, repair, or other',\n",
    "        'Credit reporting or other personal consumer reports': 'Credit reporting, repair, or other',\n",
    "        'Credit reporting, credit repair services, or other personal consumer reports': 'Credit reporting, repair, or other',\n",
    "        'Credit card': 'Credit card or prepaid card',\n",
    "        'Prepaid card': 'Credit card or prepaid card',\n",
    "        'Payday loan': 'Payday loan, title loan, personal loan, or advance loan',\n",
    "        'Payday loan, title loan, or personal loan': 'Payday loan, title loan, personal loan, or advance loan',\n",
    "        'Money transfers': 'Money transfer, virtual currency, or money service',\n",
    "        'Virtual currency': 'Money transfer, virtual currency, or money service'\n",
    "      }\n",
    "    },\n",
    "    inplace = False) # if this was a MASSIVE dataset, you may not want to make a copy but rather use df1 and inplace = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: because we have limited RAM on free tier, we'll want to clean up variables we're no longer using\n",
    "# In a real-world data science setup, we'd likely want to keep these around in case we need to go back to them for further experimentation\n",
    "del df1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('Product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.replace({\n",
    "    'Product':\n",
    "      {\n",
    "        'Debt or credit management': 'Other financial service'\n",
    "      }\n",
    "    },\n",
    "    inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2.Product.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'category_id' with encoded categories\n",
    "df2['category_id'] = df2['Product'].factorize()[0]\n",
    "category_id_df = df2[['Product', 'category_id']].drop_duplicates()\n",
    "\n",
    "# Dictionaries for future use\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Product']].values)\n",
    "\n",
    "# New dataframe\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "colors = ['grey','grey','grey','grey','grey','grey','grey','grey','grey',\n",
    "    'grey','grey','darkblue','darkblue','darkblue']\n",
    "df2.groupby('Product').Complaint.count().sort_values().plot.barh(\n",
    "    ylim=0, color=colors, title= 'NUMBER OF COMPLAINTS IN EACH PRODUCT CATEGORY\\n')\n",
    "plt.xlabel('Number of ocurrences', fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2),\n",
    "                        stop_words='english')\n",
    "\n",
    "# WARNING!!! If we use the full dataset of ~1.5 million observations, the following line of code will crash the runtime (RAM resources) on Colab's free tier.\n",
    "# One option would be to pay for a premium account, and go grab a coffee while this crunches for ~30min\n",
    "# Because the computation is also time consuming (in terms of CPU), the data was sampled to 15,000 which is sufficient to create a model\n",
    "\n",
    "df2 = df2.sample(15000, random_state=1337).copy()\n",
    "\n",
    "features = tfidf.fit_transform(df2.Complaint).toarray() # Note: this will take ~5sec\n",
    "\n",
    "labels = df2.category_id\n",
    "\n",
    "print(\"Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names_out())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"\\n==> %s:\" %(Product))\n",
    "  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n",
    "  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data (train vs test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2['Complaint'] # Collection of \"documents\" (observations of complaints)\n",
    "y = df2['Product'] # Target or the labels we want to predict (i.e., the 14 different complaints of products)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING MODELS\n",
    "\n",
    "models = [LinearSVC(), MultinomialNB(),]\n",
    "\n",
    "# 5 Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV) # Note: this line takes around 60 seconds\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING MODELS\n",
    "\n",
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='model_name', y='accuracy',\n",
    "            data=cv_df,\n",
    "            color='lightblue',\n",
    "            showmeans=True)\n",
    "plt.title(\"MEAN ACCURACY (cv = 5)\\n\", size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVALUATING MODELS\n",
    "X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features,\n",
    "                                                               labels,\n",
    "                                                               df2.index, test_size=0.25,\n",
    "                                                               random_state=1337)\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification report\n",
    "print('\\t\\t\\t\\tCLASSIFICATION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred,\n",
    "                                    target_names= df2['Product'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFUSION MATRIX\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels=category_id_df.Product.values,\n",
    "            yticklabels=category_id_df.Product.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspection of misclassifications\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 20:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual],\n",
    "                                                           id_to_category[predicted],\n",
    "                                                           conf_mat[actual, predicted]))\n",
    "\n",
    "      display(df2.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Product',\n",
    "                                                                'Complaint']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most correlated terms with each category\n",
    "\n",
    "N = 4\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "  indices = np.argsort(model.coef_[category_id])\n",
    "  feature_names = np.array(tfidf.get_feature_names_out())[indices]\n",
    "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "  print(\"\\n==> '{}':\".format(Product))\n",
    "  print(\"  * Top unigrams: %s\" %(', '.join(unigrams)))\n",
    "  print(\"  * Top bigrams: %s\" %(', '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2),\n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "model = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_complaint = \"\"\"I have been enrolled back at XXXX XXXX University in the XX/XX/XXXX. Recently, i have been harassed by \\\n",
    "Navient for the last month. I have faxed in paperwork providing them with everything they needed. And yet I am still getting \\\n",
    "phone calls for payments. Furthermore, Navient is now reporting to the credit bureaus that I am late. At this point, \\\n",
    "Navient needs to get their act together to avoid me taking further action. I have been enrolled the entire time and my \\\n",
    "deferment should be valid with my planned graduation date being the XX/XX/XXXX.\"\"\"\n",
    "print(model.predict(fitted_vectorizer.transform([new_complaint])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_complaint_2 = \"\"\"Equifax exposed my personal information without my consent, as part of their recent data breach. \\\n",
    "In addition, they dragged their feet in the announcement of the report, and even allowed their upper management to sell \\\n",
    "off stock before the announcement.\"\"\"\n",
    "print(model.predict(fitted_vectorizer.transform([new_complaint_2])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
